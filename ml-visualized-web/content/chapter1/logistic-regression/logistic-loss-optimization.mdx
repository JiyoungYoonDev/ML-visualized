---
title: 'Logistic Loss Optimization'
summary: 'Derive the gradient intuition for logistic loss and connect it to gradient descent updates.'
---

<Block
  label="Context"
  title="Logistic Regression Loss Optimization"
  description="Let's dive into the math of logistic regression and see how the loss function leads to a simple gradient descent update rule."
>
  <div className="space-y-12">
    
    {/* 1. Problem Definition */}
    <div className="grid gap-8 md:grid-cols-2 items-start">
      <div className="space-y-4">
        <div className="inline-flex items-center rounded-full bg-blue-50 px-3 py-1 text-xs font-medium text-blue-700 ring-1 ring-inset ring-blue-700/10">
          The Binary Problem
        </div>
        <p className="text-sm leading-relaxed text-muted-foreground">
          Think of this as binary classification: we have two classes, <strong>0 and 1</strong>. The answer is simply True or False. 
          When we guess, we want an Error of 0 for correct answers and a high penalty for wrong ones. 
          To achieve this, we use the <strong>Logistic Loss</strong> (Binary Cross-Entropy).
        </p>
        <div className="rounded-xl border bg-card p-4 space-y-3 shadow-sm">
          <p className="text-xs font-bold text-foreground">Key Definitions:</p>
          <ul className="text-xs space-y-2 text-muted-foreground list-disc pl-4">
            <li><LatexMath math="y" />: The true label (0 or 1)</li>
            <li><LatexMath math="p" /> (or <LatexMath math="P" />): The predicted probability via Sigmoid</li>
            <li><LatexMath math="z = w^Tx + b" />: The linear score</li>
          </ul>
        </div>
      </div>
      
      <div className="space-y-4">
        <div className="rounded-2xl bg-slate-900 p-6 shadow-xl text-white">
          <div className="mb-4 text-xs font-mono text-slate-400 uppercase tracking-widest text-center">Sigmoid Baseline</div>
          <LatexMath block math='\sigma(z)=\frac{1}{1+e^{-z}}' className="text-white [&_.katex]:text-white" />
          <p className="text-[10px] text-center text-slate-500 mt-4">Equation (1)</p>
        </div>
      </div>
    </div>

    {/* 2. Loss Function Gallery: Comparing Properties */}
    <div className="space-y-6">
      <div className="text-center">
        <h4 className="text-sm font-bold uppercase tracking-widest text-slate-400">Loss Function Gallery</h4>
        <p className="text-xs text-muted-foreground mt-1">Comparing the mathematical properties of common classifiers.</p>
      </div>

      <div className="grid gap-4 sm:grid-cols-3">
        {/* Logistic Loss */}
        <div className="rounded-xl border-2 border-blue-500 bg-white p-5 shadow-md relative overflow-hidden">
          <div className="absolute top-0 right-0 bg-blue-500 text-white text-[9px] px-2 py-1 font-bold">FOCUS</div>
          <div className="font-bold text-blue-600">Logistic Loss</div>
          <div className="my-3 text-[11px] bg-slate-50 p-2 rounded font-mono">
            <LatexMath math='l_{log}(z) = \log(1 + e^{-z})' />
          </div>
          <p className="text-[11px] text-muted-foreground">
            <strong>Convex</strong> and easy to optimize. If <LatexMath math="y^iw^Tx_i < 0" />, prediction is incorrect and loss increases.
          </p>
        </div>

        {/* Hinge Loss */}
        <div className="rounded-xl border bg-card p-5 shadow-sm">
          <div className="font-bold text-foreground">Hinge Loss</div>
          <div className="my-3 text-[11px] bg-slate-50 p-2 rounded font-mono">
            <LatexMath math='l_{hinge}(z) = \max(0, 1-z)' />
          </div>
          <p className="text-[11px] text-muted-foreground">
            Used in SVMs. Penalizes wrong predictions and those that aren't confident enough. If loss $\le 0$, it's correct and confident.
          </p>
        </div>

        {/* Exponent Loss */}
        <div className="rounded-xl border bg-card p-5 shadow-sm">
          <div className="font-bold text-foreground">Exponent Loss</div>
          <div className="my-3 text-[11px] bg-slate-50 p-2 rounded font-mono">
            <LatexMath math='l_{exp}(z) = e^{-z}' />
          </div>
          <p className="text-[11px] text-muted-foreground">
            Used in AdaBoost. <strong>Convex</strong> but more sensitive to outliers. Loss $> 1$ means prediction is incorrect.
          </p>
        </div>
      </div>

      <div className="p-4 bg-slate-50 rounded-xl border border-dashed text-center">
        <GraphPlot dataUrl="/data/loss_funcs_compare.json" />
        <p className="text-[10px] text-slate-400 mt-2 uppercase">Visualization of Loss Curves vs. Margin</p>
      </div>
    </div>

    {/* 3. The Power of Logarithms */}
    <div className="rounded-2xl border bg-blue-50/30 p-6 md:p-8">
      <div className="flex flex-col md:flex-row gap-8 items-center">
        <div className="flex-1 space-y-4">
          <h4 className="text-lg font-bold text-blue-900 flex items-center gap-2">
            Why the Log Function?
          </h4>
          <p className="text-sm leading-relaxed">
            Log transforms the loss into a manageable, <strong>convex</strong> form. It enforces a massive penalty for "confident mistakes."
            If <LatexMath math="y=1" /> but <LatexMath math="P \to 0" />, the loss approaches infinity because:
          </p>
          <div className="bg-white p-4 rounded-xl border border-blue-100 shadow-sm">
            <LatexMath block math='\begin{cases} \lim_{x \to 0^+} \log(x) = -\infty \\ \log(1) = 0 \end{cases}' />
          </div>
        </div>
        <div className="flex-none text-sm space-y-2">
           <div className="bg-white p-3 rounded-lg border border-blue-100">
              <span className="font-bold text-green-600">P = y</span> → Low Loss
           </div>
           <div className="bg-white p-3 rounded-lg border border-blue-100">
              <span className="font-bold text-red-600">P ≠ y</span> → High Loss
           </div>
        </div>
      </div>
    </div>

    {/* 4. The Gradient Descent Engine */}
    <div className="space-y-6">
      <div className="border-l-4 border-green-500 pl-4">
        <h3 className="text-xl font-black uppercase tracking-tight">The Update Engine: Gradient Descent</h3>
        <p className="text-sm text-muted-foreground italic">Iteratively updating parameters $w$ and $b$ to reduce error.</p>
      </div>

      <div className="grid gap-6 md:grid-cols-2">
        {/* Error logic */}
        <div className="bg-card border rounded-2xl p-6">
          <div className="text-xs font-bold text-green-600 uppercase mb-4 tracking-widest text-center">1. Identify the Error</div>
          <div className="text-center space-y-4">
            <LatexMath block math='error = y - P' className="text-2xl font-black" />
            <div className="text-xs space-y-2 text-muted-foreground">
              <p><strong>Positive Error:</strong> <LatexMath math="P < y" />. We must increase <LatexMath math="P" />.</p>
              <p><strong>Negative Error:</strong> <LatexMath math="P > y" />. We must decrease <LatexMath math="P" />.</p>
            </div>
          </div>
        </div>

        {/* Update Rule */}
        <div className="bg-slate-900 rounded-2xl p-6 text-white shadow-xl">
          <div className="text-xs font-bold text-blue-400 uppercase mb-4 tracking-widest text-center">2. Apply Update Rules</div>
          <div className="space-y-4">
            <div className="p-4 bg-white/5 rounded-xl border border-white/10">
              <LatexMath block math='w \leftarrow w - \eta \nabla_w \ell' className="text-white [&_.katex]:text-white" />
            </div>
            <div className="p-4 bg-white/5 rounded-xl border border-white/10">
              <LatexMath block math='b \leftarrow b - \eta \frac{\partial \ell}{\partial b}' className="text-white [&_.katex]:text-white" />
            </div>
          </div>
          <p className="text-[10px] text-slate-500 mt-4 italic text-center">
            $\eta$ (Learning Rate) controls the size of each step.
          </p>
        </div>
      </div>
    </div>

  </div>
</Block>
